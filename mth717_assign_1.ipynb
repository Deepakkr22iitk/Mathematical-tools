{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-drSv55vr15",
        "outputId": "75ebe304-e5bb-417d-f0d2-e2de26df8887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Problem a ---\n",
            "Eta: 0.1 | Minimum at: 2.9999571825692186 | Iterations: 50\n",
            "Eta: 0.5 | Minimum at: 3.0 | Iterations: 1\n",
            "\n",
            "--- Problem b ---\n",
            "Eta: 0.1 | Minimum at: -0.3854353879474503 | Iterations: 25\n",
            "Eta: 0.5 | Minimum at: nan | Iterations: 10000\n",
            "\n",
            "--- Problem c ---\n",
            "Eta: 0.1 | Minimum at: [ 0.89998207 -0.69997098] | Iterations: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-884306878.py:31: RuntimeWarning: overflow encountered in scalar power\n",
            "  grad_b = lambda x: 4 * x**3 + 2 * x + 1\n",
            "/tmp/ipython-input-884306878.py:19: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  x = x - eta * gradient\n",
            "/tmp/ipython-input-884306878.py:36: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  df_dx = 6*v[0] + 2*v[1] - 4\n",
            "/tmp/ipython-input-884306878.py:37: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  df_dy = 2*v[0] + 4*v[1] + 1\n",
            "/tmp/ipython-input-884306878.py:19: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = x - eta * gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eta: 0.5 | Minimum at: [nan nan] | Iterations: 10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(f, grad_f, x0, eta, tol=1e-4, max_iter=10000):\n",
        "    \"\"\"\n",
        "    Generic Gradient Descent Solver\n",
        "    x0: starting point (can be float or np.array)\n",
        "    eta: learning rate\n",
        "    tol: accuracy threshold for the gradient norm\n",
        "    \"\"\"\n",
        "    x = np.array(x0, dtype=float)\n",
        "    for i in range(max_iter):\n",
        "        gradient = np.array(grad_f(x), dtype=float)\n",
        "\n",
        "        # Check convergence: norm of gradient < tolerance\n",
        "        if np.linalg.norm(gradient) < tol:\n",
        "            return x, i\n",
        "\n",
        "        # Update step: x_{new} = x_{old} - eta * grad\n",
        "        x = x - eta * gradient\n",
        "\n",
        "    return x, max_iter\n",
        "\n",
        "# --- Problem Definitions ---\n",
        "\n",
        "# a) f(x) = (x-3)^2 + 2\n",
        "f_a = lambda x: (x - 3)**2 + 2\n",
        "grad_a = lambda x: 2 * (x - 3)\n",
        "\n",
        "# b) f(x) = x^4 + x^2 + x\n",
        "f_b = lambda x: x**4 + x**2 + x\n",
        "grad_b = lambda x: 4 * x**3 + 2 * x + 1\n",
        "\n",
        "# c) f(x,y) = 3x^2 + 2xy + 2y^2 - 4x + y\n",
        "f_c = lambda v: 3*v[0]**2 + 2*v[0]*v[1] + 2*v[1]**2 - 4*v[0] + v[1]\n",
        "def grad_c(v):\n",
        "    df_dx = 6*v[0] + 2*v[1] - 4\n",
        "    df_dy = 2*v[0] + 4*v[1] + 1\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "problems = [\n",
        "    (\"a\", grad_a, 0, [0.1, 0.5]),\n",
        "    (\"b\", grad_b, 1, [0.1, 0.5]),\n",
        "    (\"c\", grad_c, [0, 0], [0.1, 0.5])\n",
        "]\n",
        "\n",
        "for label, grad_func, start_pt, etas in problems:\n",
        "    print(f\"--- Problem {label} ---\")\n",
        "    for e in etas:\n",
        "        sol, iters = gradient_descent(None, grad_func, start_pt, e)\n",
        "        print(f\"Eta: {e} | Minimum at: {sol} | Iterations: {iters}\")\n",
        "    print()"
      ]
    }
  ]
}